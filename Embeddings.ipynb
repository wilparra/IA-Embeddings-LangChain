{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527bbfb3-a715-43c3-b099-37c1ac8e8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecdc0b3-58ab-4488-85b9-33ce7f3b580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "print(os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c322e-0069-488a-964d-ea8cad8976bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain==0.1.*\n",
    "#!pip install langchain-community==0.0.*\n",
    "#!pip install langchain-openai==0.1.*\n",
    "#!pip install openai\n",
    "#!pip install weaviate-client==3.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d229f-de8c-49bc-b5be-e73486a683e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_experimental -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e02878-ace5-45cd-8773-ffbcb8f0b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "with open('La-Republilca-Platon.txt', encoding='utf-8') as f:\n",
    "    repubica = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=60,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents([repubica])\n",
    "\n",
    "#print(chunks)\n",
    "# Para ver cu치ntos fragmentos se tiene, se imprime el tama침o de los chunk.\n",
    "print(f'Hay {len(chunks)} fragmentos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231eb996-9668-4a5b-915c-0e7671604f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se imprime una muestra del contenido\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a5604b-f35b-40b2-93b9-a6be7b3982b2",
   "metadata": {},
   "source": [
    "## Calcular la cantidad de tokens de un texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b9ca76-ff20-4ebd-8841-278bb96d353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def print_embedding_tokens(texts):\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-large')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    return total_tokens\n",
    "\n",
    "total_tokens = print_embedding_tokens(chunks)\n",
    "print(f'Total Tokens: {total_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a0190-c21f-477e-bdf2-4831388542db",
   "metadata": {},
   "source": [
    "## Crear Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43639b9-36e4-4589-ae4e-37bfcd80c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No se usa por costos de uso del modelo\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# embeddings = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=3072)\n",
    "# vector = embeddings.embed_query(chunks[0].page_content)\n",
    "# vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1dafa4-58f4-473f-aa54-c4fc2b720310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google.genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc4055-7db8-45ff-b80f-33d299704b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "text = chunks[0].page_content\n",
    "embeddings = client.models.embed_content(\n",
    "    model=\"text-embedding-004\",\n",
    "    contents=text,\n",
    "    config=types.EmbedContentConfig(output_dimensionality=92),\n",
    ")\n",
    "print(embeddings.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69704c47-0b61-436f-8414-b3fba02475b2",
   "metadata": {},
   "source": [
    "## Insertar el vector en weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52944fdf-a43b-4016-ac1f-daa793923bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall weaviate-client -y\n",
    "#!pip uninstall weaviate-client -y\n",
    "#!pip install weaviate-client==3.26.2\n",
    "#!pip install weaviate-client==3.26.2\n",
    "#!pip install sentence-transformers\n",
    "# !pip install --upgrade pip setuptools wheel\n",
    "#!pip install numpy\n",
    "#!pip install langchain-weaviate\n",
    "#!pip install sentence-transformers faiss-cpu\n",
    "!pip install --upgrade faiss-cpu numpy                                                                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c57e4-55f3-4ac8-b06b-8575aa944a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 1. Modelo de embeddings local\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2. Tus documentos (pueden venir de cualquier fuente)\n",
    "documentos = [\n",
    "    \"LangChain es una biblioteca para construir aplicaciones LLM.\",\n",
    "    \"FAISS es una biblioteca para b칰squedas de similitud usando vectores.\",\n",
    "    \"Transformers son modelos de deep learning como BERT o GPT.\",\n",
    "    \"La inteligencia artificial est치 transformando muchas industrias.\",\n",
    "    \"Python es un lenguaje de programaci칩n muy popular.\"\n",
    "]\n",
    "\n",
    "# 3. Crear embeddings\n",
    "embeddings = model.encode(documentos)\n",
    "\n",
    "# 4. Crear 칤ndice FAISS (칤ndice de similitud)\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# 5. Consulta\n",
    "consulta = \"쯈u칠 es LangChain?\"\n",
    "embedding_consulta = model.encode([consulta])\n",
    "\n",
    "# 6. B칰squeda en el 칤ndice\n",
    "k = 2  # top-k resultados\n",
    "distancias, indices = index.search(np.array(embedding_consulta), k)\n",
    "\n",
    "print(\"\\n游댌 Resultados para la consulta:\", consulta)\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"{i+1}. {documentos[idx]} (Distancia: {distancias[0][i]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6aebe-7ffc-40d3-8966-ea4561accc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymupdf langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc51e3-7141-428b-b44f-227213f4c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain.schema import Document\n",
    "\n",
    "def cargar_pdf_a_documentos(path_pdf):\n",
    "    documentos = []\n",
    "    with fitz.open(path_pdf) as doc:\n",
    "        for i, pagina in enumerate(doc):\n",
    "            texto = pagina.get_text()\n",
    "            if texto.strip():  # Evita p치ginas en blanco\n",
    "                documentos.append(Document(page_content=texto, metadata={\"page\": i + 1}))\n",
    "    return documentos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350a8c6-cb44-4760-88b2-dd7ad6d3c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 1. Cargar el PDF\n",
    "docs = cargar_pdf_a_documentos(\"C:/Users/wilpa/OneDrive/Escritorio/Libros/Generative-AI-with-LangChain-2024-Original.pdf\")\n",
    "\n",
    "# 2. Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 3. Crear vectorstore con FAISS\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# 4. Guardar localmente si quieres reutilizar\n",
    "vectorstore.save_local(\"vectorstore_pdfs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb611c3-f0fc-4855-9bf5-c11fe1a9e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recargar (si ya lo hab칤as guardado)\n",
    "# vectorstore = FAISS.load_local(\"vectorstore_pdfs\", embedding_model)\n",
    "\n",
    "query = \"쮺u치l es el prop칩sito del documento?\"\n",
    "resultados = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(resultados):\n",
    "    print(f\"\\nDocumento {i+1}: P치gina {doc.metadata['page']}\")\n",
    "    print(doc.page_content[:500])  # Mostrar primeros 500 caracteres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd0ff8-4bf1-4581-bf62-28eec1564518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-google-genai\n",
    "#!pip install --upgrade protobuf\n",
    "#!python -c \"import google.protobuf; print(google.protobuf.__version__)\"\n",
    "#!pip install --upgrade protobuf\n",
    "#!pip install --upgrade protobuf\n",
    "!pip install google-cloud-core google-cloud-storage google-cloud-bigquery google-api-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd358fc-c9ee-4193-9fcf-ed332fb08efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 1. Funci칩n para leer PDF y cargar documentos\n",
    "def cargar_pdf_a_documentos(path_pdf):\n",
    "    documentos = []\n",
    "    with fitz.open(path_pdf) as doc:\n",
    "        for i, pagina in enumerate(doc):\n",
    "            texto = pagina.get_text()\n",
    "            if texto.strip():\n",
    "                documentos.append(Document(page_content=texto, metadata={\"page\": i + 1}))\n",
    "    return documentos\n",
    "\n",
    "# 2. Carga documentos desde PDF\n",
    "docs = cargar_pdf_a_documentos(\"C:/Users/wilpa/OneDrive/Escritorio/Libros/Cloud-Data-Warehousing-For-Dummies-3rd-Edition.pdf\")\n",
    "\n",
    "# 3. Crear embeddings (OpenAI en este ejemplo)\n",
    "#embedding_model = OpenAIEmbeddings()\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# 4. Crear vectorstore FAISS con los documentos embebidos\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# 5. Crear modelo LLM OpenAI (puedes usar otras opciones tambi칠n)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    #model=\"gemini-1.5-pro\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "#llm = OpenAI(temperature=0)\n",
    "\n",
    "# 6. Crear chain RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())\n",
    "\n",
    "# 7. Preguntar\n",
    "query = \"쮺u치l es el prop칩sito del documento?\"\n",
    "respuesta = qa_chain.run(query)\n",
    "\n",
    "print(\"Respuesta:\", respuesta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d06db2-324e-418d-8c5a-f0b5322753fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade protobuf==6.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e988d56-c1e0-4cdb-bafa-46f22511da66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428399e9-b21c-48fd-b4e8-fe2a47c5e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c563e8-d060-435c-9b40-34dbff864895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb270c7-6060-49ef-8b64-8599cf39aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-huggingface\n",
    "#!pip install huggingface_hub transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e03656-e380-4471-b046-56696dc32383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6981ed-2325-4c16-b5d8-f3983f0f6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community faiss-cpu  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c13654-1588-47ed-bd93-ef25cf6776d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3f409-4035-43bf-b8e8-52bc0f7a0a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4ace1-9763-433c-a464-8d734e822a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e766c60-8340-4c0b-a969-b704878ed001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a55ed-edc9-4d8f-bbf3-2a598946ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1be1f-3f1e-4cc9-aa57-5844dbd6994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313423b-a3e8-470f-9f61-657fcb35ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f3f86-6e6e-490c-b4d3-5b91b450f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710e175-76f3-45ff-9a67-f55ea8d0f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install weaviate-client>=4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662847e-b14f-4f30-8b5d-5816c736e421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
