{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvSMOXuRFbRw",
    "outputId": "89c12a06-4979-4658-e849-55c18ee6eb09"
   },
   "outputs": [],
   "source": [
    "!pip install langchain_core\n",
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKkN0UKM-FIU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "messages = [\n",
    "        SystemMessage(content = \"Eres un asistente personal de IA\"),\n",
    "        HumanMessage(content = \"¿Me ayudas a preparar arroy chino?\"),\n",
    "        AIMessage(content = \"¡Claro que si!\"),\n",
    "        HumanMessage(content = \"Tengo los siguientes ingredientes: Arroz, raices, aceite de oliva, cebolla, pimenton, lomo de cerdo, pollo, jamon, camarones, salsa soja\"),\n",
    "        AIMessage(\"Aqui tienes la receta:\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlMZveQCEfXl",
    "outputId": "578476a4-f939-46d2-a864-06572c519bd5"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Introduce tu clave API de Google AI: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9X7X1NrPJ5at"
   },
   "outputs": [],
   "source": [
    "!pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNGPLIbRKDNF"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    #model=\"gemini-1.5-pro\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56sXHJr0KRs6",
    "outputId": "43984d0f-fb7e-44cd-97f0-cee99ad36206"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Eres un asistente personal de IA.\",\n",
    "    ),\n",
    "    (\"human\", \"¿Me ayudas a preparar arroz chino?\"),\n",
    "    (\"ai\", \"¡Claro que si!\"),\n",
    "    (\"human\", \"Tengo los siguientes ingredientes: Arroz, raices, aceite de oliva, cebolla, pimenton, lomo de cerdo, pollo, jamon, camarones, salsa soja\"),\n",
    "    (\"ai\", \"Aqui tienes la receta:\")\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg\n",
    "\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lV7wsgKFLBnG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sNmCzrgM3jJ"
   },
   "source": [
    "Encadenar el Modelo con Plantillas de Prompts\n",
    "Podemos mejorar la funcionalidad del modelo encadenándolo con plantillas de prompts. Esto permite adaptar el comportamiento del modelo de forma dinámica, por ejemplo, para traducciones entre diferentes idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYSJFi8fM4sd",
    "outputId": "f98d823d-a3f0-4154-a90a-0b3c122b6865"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Crear una plantilla de prompt para traducciones\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Eres un asistente útil que traduce {input_language} a {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Encadenar la plantilla con el modelo de Google AI\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invocar la cadena con parámetros específicos\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"Spanish\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFmKuKWyM_zl"
   },
   "source": [
    "Configuración de Seguridad en los Modelos Gemini\n",
    "Los modelos Gemini de Google AI tienen configuraciones de seguridad por defecto que ayudan a prevenir la generación de contenido dañino. Sin embargo, estas configuraciones pueden ser ajustadas para adaptarse mejor a las necesidades de tu aplicación.\n",
    "\n",
    "Por ejemplo, si estás recibiendo muchos \"Safety Warnings\" en tus respuestas, puedes modificar los umbrales de bloqueo para categorías específicas de contenido:\n",
    "\n",
    "Este código ajusta la configuración de seguridad para permitir contenido que normalmente podría estar bloqueado por defecto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EJsswS1NAeL"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import (\n",
    "    ChatGoogleGenerativeAI,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "\n",
    "# Desactivar el bloqueo para contenido peligroso\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    safety_settings={\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26FKw_JjNlLr"
   },
   "source": [
    "#Conclusión\n",
    "\n",
    "##Los modelos de Google ofrecen una solución robusta para construir aplicaciones de chat avanzadas, desde la configuración básica hasta la generación de respuestas y el uso de plantillas de prompts, hemos explorado cómo ajustar las configuraciones de seguridad para personalizar el comportamiento del modelo según las necesidades de tu aplicación.utilizando los modelos de Google AI con LangChain específicamente con Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ieC1lJoNcEa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1viUM1RQMG0"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "kok0K7eGReT4",
    "outputId": "e7e52f32-f021-4e0b-af8d-e728cdae08ba"
   },
   "outputs": [],
   "source": [
    "messages=\"¿Cuanto es 2 * 75?\"\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqIT1k-1TiOf"
   },
   "source": [
    "Ahora lo entrenamos para que innfiera datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TB8l933UTmWW"
   },
   "outputs": [],
   "source": [
    "# Con estos ejemplos se entren el modelo\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [{'input':'2 ⌂ 2','output':'4'},\n",
    "            {'input':'2 ⌂ 3','output':'5'}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtPEAmSKUaVS"
   },
   "outputs": [],
   "source": [
    "# Se crea el template de ejemplo\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8j6-_1VUxb9"
   },
   "outputs": [],
   "source": [
    "# Se une el template con los ejemplos\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0annxzUVAxI",
    "outputId": "a8054c27-c7b4-4f1d-f491-f1baf4b401ff"
   },
   "outputs": [],
   "source": [
    "# Se genera respuesta\n",
    "#print(few_shot_prompt.invoke({\"input\":\"2 ⌂ 4\"}))\n",
    "#print(few_shot_prompt.invoke({\"input\":\"2 ⌂ 8\"}).to_messages())\n",
    "\n",
    "print(few_shot_prompt.invoke({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcmDUcj2VnJN"
   },
   "outputs": [],
   "source": [
    "# Se construye el prompt principal con un template de entrada para el usuario\n",
    "main_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('system','Eres un mago de las matematicas.'),\n",
    "     few_shot_prompt,\n",
    "     (\"human\", \"{input}\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Qq6OZssVWAZu",
    "outputId": "132ffd16-f983-44f0-8bfb-809963d1f3f8"
   },
   "outputs": [],
   "source": [
    "# Se integra el main_prompt con el modelo\n",
    "chain = main_prompt | llm\n",
    "\n",
    "# Se invoca la cadena y se pasa la entrada para que la IA interprete\n",
    "#chain.invoke({\"input\":\"2 ⌂ 9\"}).content\n",
    "chain.invoke({\"input\":\"¿Cuanto es 5 ⌂ 9 ?\"}).content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77p_zDL1YHRT"
   },
   "source": [
    "Ahora se va a mejorar la respuesta con templates de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDZqW7s0YLHX"
   },
   "outputs": [],
   "source": [
    "# Crear una plantilla de prompt para traducciones\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",\"Traduce lo siguiente al {language}:\"),\n",
    "     (\"human\", \"{text}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Wz1V8cVaDmk"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = prompt_template | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s01KywNQYhUE"
   },
   "outputs": [],
   "source": [
    "response = chain.invoke({'language':'Italian','text':'hello'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GqZjBR9a7w6",
    "outputId": "b8e6f30d-aac7-4b09-bf4b-3d10b58ea7bc"
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqKcvQVbeGOk"
   },
   "outputs": [],
   "source": [
    "# Ahora otros ejemplos manteniendo el historial:\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "if not chat_history:\n",
    "    chat_history.append(SystemMessage(content = \"Eres un asistente personal de IA\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9PwUkxOgiMA",
    "outputId": "3803f6f7-f79a-4f17-b804-9db244f30940"
   },
   "outputs": [],
   "source": [
    "query = input('Haz una pregunta: ')\n",
    "\n",
    "chat_history.append(HumanMessage(content = query))\n",
    "response = llm.invoke(chat_history).content\n",
    "chat_history.append(AIMessage(content=response))\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcIMXc-ofnGz",
    "outputId": "bdee04b8-bba5-4dc9-8498-cd9148fc0b1b"
   },
   "outputs": [],
   "source": [
    "for message in chat_history:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGPXNqVrhIE-",
    "outputId": "c1310533-49d5-4d58-e4b0-16cd10bdbfd1"
   },
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-pacB3khi27"
   },
   "source": [
    "Ahora se va a trabajar con Runnable, OutputParser y Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U75aBdx8hKvp",
    "outputId": "eed89b59-46bb-4cab-8618-87ea022a5984"
   },
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dS2GCg6Gh3Qv",
    "outputId": "1538835c-2671-4054-a884-9d4c1c1386e6"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcFgfmo4ihJB",
    "outputId": "febac9f4-497a-4255-f61e-78e016809625"
   },
   "outputs": [],
   "source": [
    "# Pasamos 10 al primer lambda y este lambda lo pasa al segundo\n",
    "sequence.invoke(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPRKeJa2iiV5"
   },
   "outputs": [],
   "source": [
    "sequence = RunnableLambda(lambda x: x + 1) | {\n",
    "    'index_1':RunnableLambda(lambda x: x * 2),\n",
    "    'index_2':RunnableLambda(lambda x: x * 5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_QpUO85iz4p",
    "outputId": "188d8fbd-768d-46f0-8f5e-831209113f14"
   },
   "outputs": [],
   "source": [
    "sequence.invoke(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWpdGdWDjJWW"
   },
   "outputs": [],
   "source": [
    "# Ahora vamo a obtener respuestas en formato Json\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    #model=\"gemini-1.5-pro\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D57eVb7gkLnU"
   },
   "outputs": [],
   "source": [
    "joke_query = 'Tell me a joke'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVW8nT2ajrVb"
   },
   "outputs": [],
   "source": [
    "parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQgu-xn-jzu8"
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template='Answer the user query. \\n {format_instructions} \\n Query: {query}',\n",
    "    input_variables=['query'],\n",
    "    partial_variables={'format_instructions':parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fL_VHcHcj3x1"
   },
   "outputs": [],
   "source": [
    "\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNUoHliYk6ci"
   },
   "outputs": [],
   "source": [
    "response = chain.invoke({'query':joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmSTg_VWlGd1",
    "outputId": "38d52d69-bed4-4341-9701-60bdcb9e71a8"
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K1C_XjlmKKO"
   },
   "source": [
    "# Ahora vamos a probar el Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7_yGn_jmNuW",
    "outputId": "742d6ec3-1e47-4466-ed8a-32f104302689"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for s in chain.stream({'query':joke_query}):\n",
    "    print(s)\n",
    "    time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRcsNO2ymtte",
    "outputId": "5d59308b-f830-4023-c696-b87592f0645b"
   },
   "outputs": [],
   "source": [
    "chunks = []\n",
    "async for chunk in model.astream(joke_query):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end='', flush=True)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otTlA4zfnbvl"
   },
   "source": [
    "Ahora se va a crear un chatbot inteligente\n",
    "\n",
    "1. Configuracion\n",
    "2. Sistema de memoria\n",
    "3. Interaccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1MDJ_ATnfGD"
   },
   "outputs": [],
   "source": [
    "# Para almacenar la información historica de la conversacion\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory\n",
    ")\n",
    "from langchain_core.runnables import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ED33AcFq3Tp"
   },
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "  if session_id not in store:\n",
    "    store[session_id] = InMemoryChatMessageHistory()\n",
    "  return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCp5Mb8Mrd_X"
   },
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "  model,\n",
    "  get_session_history\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0p6qYrvlrwqK"
   },
   "outputs": [],
   "source": [
    "config = {'configurable':{'session_id':'abc2'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbWQMunGr_SR"
   },
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "  [HumanMessage(content='Hola, soy William')],\n",
    "  config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ejScgj2CtZy1",
    "outputId": "ec79ac6d-a79d-40b1-e976-1303bcea9f37"
   },
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rH6iLSlEtguf"
   },
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "  [HumanMessage(content='¿Cuál es mi nombre?')],\n",
    "  config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Ye6FJnIFtoQQ",
    "outputId": "f3e0a6bb-a6fa-4e33-a088-4399ad5b0019"
   },
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMIau-ckttxJ",
    "outputId": "f32a64be-904d-441d-f3ff-d09f6aef84e9"
   },
   "outputs": [],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KH48t-U_v2L-"
   },
   "outputs": [],
   "source": [
    "#Ahora cambiamos de ID\n",
    "config = {'configurable':{'session_id':'abc3'}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RY_WxYRv_Sp"
   },
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "  [HumanMessage(content='¿Cuál es mi nombre?')],\n",
    "  config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nVHKZQn6wAuJ",
    "outputId": "aa3d4a92-704d-4b65-c323-2eb0a4404f69"
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8895yJjwEvw"
   },
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "  [HumanMessage(content='Hola, soy Alejandro')],\n",
    "  config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-ETcxbMwLyZ",
    "outputId": "376444c0-a1d9-4472-8a2b-62dbc8edfc6a"
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2TLJZBMwOUX"
   },
   "outputs": [],
   "source": [
    "# Vuelvo a preguntar quien soy\n",
    "response = with_message_history.invoke(\n",
    "  [HumanMessage(content='¿Cuál es mi nombre?')],\n",
    "  config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "aRL9FVHVwR_a",
    "outputId": "a640479b-26f8-401d-d9d4-b09a8061bc0a"
   },
   "outputs": [],
   "source": [
    "# Se pinta la respuesta y ahora si me dice quien es el que pregunta\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLYfOXkfwUyL",
    "outputId": "4206b578-7d34-4c9c-eb7e-dd4c05524337"
   },
   "outputs": [],
   "source": [
    "# En el almacen estan las dos conversaciones con todos los usuarios\n",
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqNBuJl2w5mV"
   },
   "source": [
    "Ahora le vamos a dar personalidad mediante un prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZulIkFXwR2b"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\n",
    "        'system','Eres un asistente util. Responda todas las preguntas con la mejor habilidad'\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name='messages')\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luxoHeORyD9f"
   },
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "6fN7fWb1yNlo",
    "outputId": "5900efb3-5964-418d-917e-1351b05bccde"
   },
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "  [HumanMessage(content='Hola, soy William')],\n",
    "  config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "C_i-R5ekyfgj",
    "outputId": "7301718b-e3e0-4d2e-8289-83f7eddccc63"
   },
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "  [HumanMessage(content='Cuál es tu nombre?')],\n",
    "  config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkepnKs3yoRa"
   },
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "  chain,\n",
    "  get_session_history\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cKx8xZiywXz"
   },
   "outputs": [],
   "source": [
    "config = {'configurable':{'session_id':'abc5'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "LJMcgh_yy27j",
    "outputId": "02261214-954f-48a6-ef68-06c253d647d8"
   },
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "  [HumanMessage(content='¿Cual es tu nombre?')],\n",
    "  config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ayj_Kil2zKM5",
    "outputId": "d9917506-718a-44eb-ad74-e6b8c9af083d"
   },
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "  [HumanMessage(content='Mi nombre es Bob')],\n",
    "  config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7nlD7eKdzUBE",
    "outputId": "dd5f0dd1-1030-4714-9dc7-2c5aa22aafef"
   },
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "  [HumanMessage(content='¿Cuál es mi nombre?')],\n",
    "  config=config\n",
    ")\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
